# Dissertation

ASR systems report much higher error rates when used with ‘accented’ speech, thus, many ASR researchers and developers pursue Accent Recognition from a both an ethical and a quality assurance standpoint in order to fulfill their responsibility to their users to ensure all experiences with ASR technology are equal, regardless of accent. They hope to achieve this either by using AR to elucidate distinguishing acoustic features of accents that can be incorporated into ASR research or by directly incorporating an accent classification sub-task into an ASR model. Working on the assumption that a classifier that is more robust to non-homogeneous data will generalise better on real-life examples, this project aims to expand and test previous accent classification findings on the large non-homogeneous dataset Common Voice 13.0 - the largest labeled corpora of accented speech to date. This project will be one of few works in the AR field that performs accent classification using a large dataset. It also provides a direct comparison of the effectiveness of a bespoke CNN model with fine tuning a large pre-trained acoustic model (Wav2Vec2) on Common Voice. Overall, when evaluated on unseen data, the final models report a classification accuracy of 55% for the CNN model, surpassing previous CNN benchmarks for Common Voice, and 75% for the Wav2Vec2 model.
